{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Market_data-extraction_sentimental-analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arnavdas/Market-data-extraction-sentimental-analysis/blob/master/Market_data_extraction_sentimental_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0RNS5EGPSo3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import os\n",
        "os.chdir('C:/Users/LENOVO/Desktop')\n",
        "from lxml import html\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import textstat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# reads the links and positive-negative words\n",
        "df = pd.read_excel(r'C:\\Users\\LENOVO\\Desktop\\Internshala\\LoughranMcDonald_MasterDictionary_2018.xlsx') #for an earlier version of Excel, you may need to use the file extension of 'xls'\n",
        "df1 = pd.read_excel(r'C:\\Users\\LENOVO\\Desktop\\Internshala\\cik_list - Copy.xlsx') #for an earlier version of Excel, you may need to use the file extension of 'xls'\n",
        "#print (df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PKCsevgPSo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SYLLABLE SEARCHING Function\n",
        "from nltk.corpus import cmudict\n",
        "dk = cmudict.dict()\n",
        "def nsyl(word):\n",
        "    #print([list(y for y in x if y[-1].isdigit()) for x in dk[word.lower()]] )\n",
        "    return [len(list(y for y in x if y[-1].isdigit())) for x in dk[word.lower()]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF1aBgjLPSo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# READING ALL TEXT FILES OF STOPWORDS AND SETTING UP A DICT FOR THEM\n",
        "stopw = {}; save = {}\n",
        "os.chdir(\"C:/Users/LENOVO/Desktop/Internshala\")\n",
        "for file in os.listdir(\"C:/Users/LENOVO/Desktop/Internshala\"):\n",
        "    if file.endswith(\".txt\"):\n",
        "        #print(str(file))\n",
        "        if str(file) != 'output.txt':\n",
        "            file1 = open(str(file),\"r\")\n",
        "            for x in file1:\n",
        "                stopw[x.split()[0]] = 0\n",
        "        else:\n",
        "            file2 = open(str(file),\"r\")\n",
        "            for x in file2:\n",
        "                save[x.split()[0]] = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRES8e_FPSpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SETTING UP DICT FOR + AND - WORDS\n",
        "dff = df.to_numpy(); dff_dic = {}\n",
        "for i in range(len(dff)):\n",
        "    #kk = np.where(df==i.upper())[0][0]\n",
        "    dff_dic[dff[i][0]] = [df.iloc()[i]['Negative'], df.iloc()[i]['Positive'], df.iloc()[i]['Uncertainty'],  df.iloc()[i]['Constraining']]\n",
        "#dff_dic  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB1v9c2yPSpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def saver(save, k, neg=np.nan, pos=np.nan, cln=np.nan, syl=np.nan, unc=np.nan, const=np.nan, sents=np.nan):\n",
        "    if k == 1:\n",
        "        qt = 'mda'\n",
        "    elif k == 3:\n",
        "        qt = 'qqdmr'\n",
        "    elif k == 2:\n",
        "        qt = 'rf'\n",
        "    asl = cln/sents\n",
        "    pcw = syl/cln\n",
        "    fi = 0.4*(asl + pcw)\n",
        "    save[qt+'_positive_score'].append(pos)#1\n",
        "    save[qt+'_positive_word_proportion'].append(pos/cln)#2\n",
        "    save[qt+'_negative_score'].append(neg)#3\n",
        "    save[qt+'_negative_word_proportion'].append(neg/cln)#4\n",
        "    save[qt+'_polarity_score'].append((pos - neg)/(pos + neg + 0.000001))#5 \n",
        "    save[qt+'_average_sentence_length'].append(asl)#6\n",
        "    save[qt+'_complex_word_count'].append(syl)#7\n",
        "    save[qt+'_constraining_word_proportion'].append(const/cln)#8\n",
        "    save[qt+'_percentage_of_complex_words'].append(pcw)#9\n",
        "    save[qt+'_word_count'].append(cln)#10\n",
        "    save[qt+'_fog_index'].append(fi)#11\n",
        "    save[qt+'_uncertainty_word_proportion'].append(unc/cln)#12\n",
        "    save[qt+'_uncertainty_score'].append(unc)#13\n",
        "    save[qt+'_constraining_score'].append(const)#14"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdLIoDiYPSpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SENTIMENTAL ANALYSIS, POSITIVITY SOCRE, NEGATIVITY SCORE, POLARITY\n",
        "def analysis_part(par, stopw, dff_dic, save):\n",
        "    #print(par)\n",
        "    tok = {}; const_count = 0\n",
        "    for jj in range(1, 4):\n",
        "        if len(par[jj]) != 0:\n",
        "            for i in par[jj]:\n",
        "                neg = 0; pos = 0; cln = 0; syl = 0; unc = 0; const = 0\n",
        "                for j in nltk.word_tokenize(i):\n",
        "                    if j.isalpha():# chekcs for only letters\n",
        "                        try :# checks if the word is a stopword or not\n",
        "                            if stopw[j.upper()]== 0 : # is a stopword\n",
        "                                pass\n",
        "                        except:# isn't a stopword\n",
        "                            try:# checks if a word is atleast 2-syllable\n",
        "                                if nsyl(j)[0] > 2:\n",
        "                                    syl+=1\n",
        "                            except:\n",
        "                                pass\n",
        "                            try:#checks for pos, neg, unc, const scores\n",
        "                                cln+=1\n",
        "                                if dff_dic[j.upper()][0] > 0:\n",
        "                                    neg+=1\n",
        "                                elif dff_dic[j.upper()][1] > 0:\n",
        "                                    pos+=1\n",
        "                                unc+= dff_dic[j.upper()][2]\n",
        "                                const+= dff_dic[j.upper()][3]\n",
        "                                if dff_dic[j.upper()][3] > 0:\n",
        "                                    const_count+=1\n",
        "                            except:\n",
        "                                pass\n",
        "            if cln == 0:\n",
        "                saver(save, jj)\n",
        "            else:\n",
        "                sents = textstat.sentence_count(\" \".join(par[jj]))\n",
        "                saver(save, jj, neg, pos, cln, syl, unc, const, sents)        \n",
        "        else:\n",
        "            saver(save, jj)\n",
        "    save['constraining_words_whole_report'].append(const_count)\n",
        "            \n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwBsqz-PPSpN",
        "colab_type": "code",
        "colab": {},
        "outputId": "abcd8836-25d3-4ff1-e4f9-815302734229"
      },
      "source": [
        "atm = re.compile(\".*Management's Discussions* and Analysis.*\", re.IGNORECASE)\n",
        "atm1 = re.compile(\".*Risk Factors*.*\", re.IGNORECASE)\n",
        "atm2 = re.compile(\".*Quantitative and Qualitative Disclosures* about Markets* Risks*.*\", re.IGNORECASE)\n",
        "mat = re.compile(\"item{1} \\d{1}.*\\.{1}\", re.IGNORECASE )\n",
        "part = re.compile(\"PART +[I*V*]+\")\n",
        "tam = re.compile(\"<page>\", re.IGNORECASE)\n",
        "table = re.compile(\"<\\W*TABLE>\"); cntr = 0\n",
        "\n",
        "for wez in df1['Links'].to_numpy():\n",
        "    cntr+=1\n",
        "    wez = ''.join(wez.strip().split())\n",
        "    pg = requests.get(wez)\n",
        "    sup = BeautifulSoup(pg.text, 'html.parser')\n",
        "    par = {1:[], 2:[], 3:[]}; nb = 0# not broken\n",
        "    if 'page' in [tag.name for tag in sup.find_all()]:\n",
        "        for i in range(len(sup.page.contents)):# searches only contents in pages tag\n",
        "            fr = nltk.tokenize.line_tokenize(str(sup.page.contents[i])); y = len(fr)\n",
        "            for j in range(y):# searches amongst all new lines in page tags only\n",
        "                if mat.match(fr[j]) != None or nb != 0 :# searches for 'item /d.' in fitst place only\n",
        "                    if atm.search(fr[j]) != None or nb == 1:# Management's Discussion and Analysis\n",
        "                        nb = 0\n",
        "                        for k in range(j+1, y):# inner loop to find only this section specific\n",
        "                            if mat.match(fr[k]) != None or part.search(fr[k]) != None:# searches untill there's no next ITEM X.\n",
        "                                break\n",
        "                            elif k == y-1:\n",
        "                                nb = 1\n",
        "                            else:\n",
        "                                par[1].append(fr[k])\n",
        "                    elif atm1.search(fr[j]) != None or nb == 2:# Risk factors\n",
        "                        nb = 0\n",
        "                        for k in range(j+1, y):\n",
        "                            if mat.match(fr[k]) != None or part.search(fr[k]) != None:\n",
        "                                break\n",
        "                            elif k == y-1:\n",
        "                                nb = 2\n",
        "                            else:\n",
        "                                par[2].append(fr[k])\n",
        "                    elif atm2.search(fr[j]) != None or nb == 3:# Quantitative and Qualitative Disclosures about Market Risk   \n",
        "                        nb = 0\n",
        "                        for k in range(j+1, y):\n",
        "                            if mat.match(fr[k]) != None or part.search(fr[k]) != None:\n",
        "                                break\n",
        "                            elif k == y-1:\n",
        "                                nb = 3\n",
        "                            else:\n",
        "                                par[3].append(fr[k])\n",
        "                if nb != 0 :\n",
        "                    break\n",
        "        #print('cntr:',cntr)\n",
        "        analysis_part(par, stopw, dff_dic, save)\n",
        "    else:\n",
        "        #print('cntr:',cntr)\n",
        "        save['constraining_words_whole_report'].append(np.nan)\n",
        "        for i in range(1, 4):\n",
        "            saver(save, i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cntr: 1\n",
            "cntr: 2\n",
            "cntr: 3\n",
            "cntr: 4\n",
            "cntr: 5\n",
            "cntr: 6\n",
            "cntr: 7\n",
            "cntr: 8\n",
            "cntr: 9\n",
            "cntr: 10\n",
            "cntr: 11\n",
            "cntr: 12\n",
            "cntr: 13\n",
            "cntr: 14\n",
            "cntr: 15\n",
            "cntr: 16\n",
            "cntr: 17\n",
            "cntr: 18\n",
            "cntr: 19\n",
            "cntr: 20\n",
            "cntr: 21\n",
            "cntr: 22\n",
            "cntr: 23\n",
            "cntr: 24\n",
            "cntr: 25\n",
            "cntr: 26\n",
            "cntr: 27\n",
            "cntr: 28\n",
            "cntr: 29\n",
            "cntr: 30\n",
            "cntr: 31\n",
            "cntr: 32\n",
            "cntr: 33\n",
            "cntr: 34\n",
            "cntr: 35\n",
            "cntr: 36\n",
            "cntr: 37\n",
            "cntr: 38\n",
            "cntr: 39\n",
            "cntr: 40\n",
            "cntr: 41\n",
            "cntr: 42\n",
            "cntr: 43\n",
            "cntr: 44\n",
            "cntr: 45\n",
            "cntr: 46\n",
            "cntr: 47\n",
            "cntr: 48\n",
            "cntr: 49\n",
            "cntr: 50\n",
            "cntr: 51\n",
            "cntr: 52\n",
            "cntr: 53\n",
            "cntr: 54\n",
            "cntr: 55\n",
            "cntr: 56\n",
            "cntr: 57\n",
            "cntr: 58\n",
            "cntr: 59\n",
            "cntr: 60\n",
            "cntr: 61\n",
            "cntr: 62\n",
            "cntr: 63\n",
            "cntr: 64\n",
            "cntr: 65\n",
            "cntr: 66\n",
            "cntr: 67\n",
            "cntr: 68\n",
            "cntr: 69\n",
            "cntr: 70\n",
            "cntr: 71\n",
            "cntr: 72\n",
            "cntr: 73\n",
            "cntr: 74\n",
            "cntr: 75\n",
            "cntr: 76\n",
            "cntr: 77\n",
            "cntr: 78\n",
            "cntr: 79\n",
            "cntr: 80\n",
            "cntr: 81\n",
            "cntr: 82\n",
            "cntr: 83\n",
            "cntr: 84\n",
            "cntr: 85\n",
            "cntr: 86\n",
            "cntr: 87\n",
            "cntr: 88\n",
            "cntr: 89\n",
            "cntr: 90\n",
            "cntr: 91\n",
            "cntr: 92\n",
            "cntr: 93\n",
            "cntr: 94\n",
            "cntr: 95\n",
            "cntr: 96\n",
            "cntr: 97\n",
            "cntr: 98\n",
            "cntr: 99\n",
            "cntr: 100\n",
            "cntr: 101\n",
            "cntr: 102\n",
            "cntr: 103\n",
            "cntr: 104\n",
            "cntr: 105\n",
            "cntr: 106\n",
            "cntr: 107\n",
            "cntr: 108\n",
            "cntr: 109\n",
            "cntr: 110\n",
            "cntr: 111\n",
            "cntr: 112\n",
            "cntr: 113\n",
            "cntr: 114\n",
            "cntr: 115\n",
            "cntr: 116\n",
            "cntr: 117\n",
            "cntr: 118\n",
            "cntr: 119\n",
            "cntr: 120\n",
            "cntr: 121\n",
            "cntr: 122\n",
            "cntr: 123\n",
            "cntr: 124\n",
            "cntr: 125\n",
            "cntr: 126\n",
            "cntr: 127\n",
            "cntr: 128\n",
            "cntr: 129\n",
            "cntr: 130\n",
            "cntr: 131\n",
            "cntr: 132\n",
            "cntr: 133\n",
            "cntr: 134\n",
            "cntr: 135\n",
            "cntr: 136\n",
            "cntr: 137\n",
            "cntr: 138\n",
            "cntr: 139\n",
            "cntr: 140\n",
            "cntr: 141\n",
            "cntr: 142\n",
            "cntr: 143\n",
            "cntr: 144\n",
            "cntr: 145\n",
            "cntr: 146\n",
            "cntr: 147\n",
            "cntr: 148\n",
            "cntr: 149\n",
            "cntr: 150\n",
            "cntr: 151\n",
            "cntr: 152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKpUHs9FPSpT",
        "colab_type": "code",
        "colab": {},
        "outputId": "1de9e568-745b-405d-9bb6-542e3ae0491f"
      },
      "source": [
        "set([len(save[i]) for i in save.keys()])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{152}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWLX8ZZfPSpW",
        "colab_type": "code",
        "colab": {},
        "outputId": "8a6e1357-fd04-40ce-aea0-264246c20baa"
      },
      "source": [
        "for i in save.keys():\n",
        "    df1[i] = save[i]\n",
        "df1.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'Links', 'Unnamed: 6',\n",
              "       'mda_positive_score', 'mda_negative_score', 'mda_polarity_score',\n",
              "       'mda_average_sentence_length', 'mda_percentage_of_complex_words',\n",
              "       'mda_fog_index', 'mda_complex_word_count', 'mda_word_count',\n",
              "       'mda_uncertainty_score', 'mda_constraining_score',\n",
              "       'mda_positive_word_proportion', 'mda_negative_word_proportion',\n",
              "       'mda_uncertainty_word_proportion', 'mda_constraining_word_proportion',\n",
              "       'qqdmr_positive_score', 'qqdmr_negative_score', 'qqdmr_polarity_score',\n",
              "       'qqdmr_average_sentence_length', 'qqdmr_percentage_of_complex_words',\n",
              "       'qqdmr_fog_index', 'qqdmr_complex_word_count', 'qqdmr_word_count',\n",
              "       'qqdmr_uncertainty_score', 'qqdmr_constraining_score',\n",
              "       'qqdmr_positive_word_proportion', 'qqdmr_negative_word_proportion',\n",
              "       'qqdmr_uncertainty_word_proportion',\n",
              "       'qqdmr_constraining_word_proportion', 'rf_positive_score',\n",
              "       'rf_negative_score', 'rf_polarity_score', 'rf_average_sentence_length',\n",
              "       'rf_percentage_of_complex_words', 'rf_fog_index',\n",
              "       'rf_complex_word_count', 'rf_word_count', 'rf_uncertainty_score',\n",
              "       'rf_constraining_score', 'rf_positive_word_proportion',\n",
              "       'rf_negative_word_proportion', 'rf_uncertainty_word_proportion',\n",
              "       'rf_constraining_word_proportion', 'constraining_words_whole_report'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAV6sQcHPSpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.to_excel('output.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}